name: "Llama.cpp LLM Server"
version: "1.6.0"
slug: llamacpp_llm
description: "Server LLM locale con integrazione completa Home Assistant - Accedi a entit√†, servizi e dispositivi"
arch:
  - aarch64
  - amd64
url: "https://github.com/ggml-org/llama.cpp"
startup: services
boot: auto
init: false
host_network: false
hassio_api: true
hassio_role: default
ports:
  8080/tcp: 8080
  5000/tcp: 5000
ports_description:
  8080/tcp: "API Server Llama.cpp (OpenAI compatible)"
  5000/tcp: "Home Assistant Integration API"
options:
  model_url: "https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf"
  model_name: "gemma-3-1b-it-Q4_K_M"
  context_size: 2048
  threads: 4
  gpu_layers: 0
  parallel_requests: 1
  log_level: "info"
schema:
  model_url: url
  model_name: str
  context_size: int(512,32768)
  threads: int(1,32)
  gpu_layers: int(0,100)
  parallel_requests: int(1,8)
  log_level: list(debug|info|warning|error)
# image: "ghcr.io/home-assistant/{arch}-addon-llamacpp"  # Commentato per build locale
