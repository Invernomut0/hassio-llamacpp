name: "Llama.cpp LLM Server"
version: "1.0.1"
slug: llamacpp_llm
description: "Server LLM locale basato su llama.cpp con API compatibile OpenAI per Home Assistant"
arch:
  - aarch64
  - amd64
url: "https://github.com/ggml-org/llama.cpp"
startup: services
boot: auto
init: false
host_network: false
ports:
  8080/tcp: 8080
ports_description:
  8080/tcp: "API Server HTTP (OpenAI compatible)"
options:
  model_url: "https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf"
  model_name: "gemma-3-1b-it-Q4_K_M"
  context_size: 2048
  threads: 4
  gpu_layers: 0
  parallel_requests: 1
  log_level: "info"
schema:
  model_url: url
  model_name: str
  context_size: int(512,32768)
  threads: int(1,32)
  gpu_layers: int(0,100)
  parallel_requests: int(1,8)
  log_level: list(debug|info|warning|error)
# image: "ghcr.io/home-assistant/{arch}-addon-llamacpp"  # Commentato per build locale
