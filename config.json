{
  "name": "Llama.cpp LLM Server",
  "version": "1.0.0",
  "slug": "llamacpp_llm",
  "description": "Local LLM server based on llama.cpp with OpenAI-compatible API",
  "url": "https://github.com/ggml-org/llama.cpp",
  "arch": ["aarch64", "amd64"],
  "startup": "services",
  "boot": "auto",
  "init": false,
  "map": ["config:rw", "share:rw"],
  "options": {
    "model_url": "https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf",
    "model_name": "gemma-3-1b-it-Q4_K_M",
    "context_size": 2048,
    "threads": 4,
    "gpu_layers": 0,
    "parallel_requests": 1,
    "log_level": "info"
  },
  "schema": {
    "model_url": "url",
    "model_name": "str",
    "context_size": "int(512,32768)",
    "threads": "int(1,32)",
    "gpu_layers": "int(0,100)",
    "parallel_requests": "int(1,8)",
    "log_level": "list(debug|info|warning|error)"
  },
  "ports": {
    "8080/tcp": 8080,
    "5000/tcp": 5000
  },
  "ports_description": {
    "8080/tcp": "LLM API Server (OpenAI compatible)",
    "5000/tcp": "Home Assistant Service API"
  },
  "image": "ghcr.io/home-assistant/{arch}-addon-llamacpp"
}
