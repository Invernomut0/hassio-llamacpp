# ðŸ”§ FIX: Model Non Capisce il Contesto Home Assistant

## ðŸš¨ Problema

Il modello risponde con "Come modello linguistico, non ho accesso a informazioni personali" quando chiedi informazioni sulle tue entitÃ  Home Assistant (luci, switch, sensori, etc.).

## ðŸŽ¯ Causa Radice

**I modelli piccoli (<3B parametri) non hanno capacitÃ  sufficienti per:**
1. Seguire istruzioni complesse nel system prompt
2. Interpretare correttamente il contesto HA fornito
3. Ragionare su stati delle entitÃ 
4. Applicare le istruzioni a dati strutturati

**Modelli come Gemma 3 1B (1 miliardo di parametri) sono troppo limitati per questo task.**

## âœ… Soluzioni

### Soluzione 1: Usare un Modello PiÃ¹ Grande (RACCOMANDATO)

#### Modello Consigliato: Home-Llama-3.2-3B (Specializzato HA) â­

Questo modello Ã¨ **specificamente fine-tuned per Home Assistant** - Ã¨ la scelta perfetta!

1. **Apri Home Assistant**
2. **Vai su: Supervisor â†’ Llama.cpp LLM Server â†’ Configuration**
3. **Modifica questi parametri:**

```yaml
model_url: https://huggingface.co/acon96/Home-Llama-3.2-3B-GGUF/resolve/main/home-llama-3.2-3b-Q4_K_M.gguf
model_name: home-llama-3.2-3b-Q4_K_M
context_size: 4096
```

4. **Salva e Riavvia l'addon**
5. **Attendi il download del nuovo modello (~2 GB)**

#### Requisiti Hardware
- RAM: minimo 4 GB
- Storage: 2 GB liberi
- CPU: Qualsiasi (GPU opzionale ma consigliata)

#### PerchÃ© Home-Llama-3.2-3B?
âœ… **Addestrato specificamente per Home Assistant** ðŸŽ¯  
âœ… 3 miliardi di parametri (3x piÃ¹ intelligente di Gemma 1B)  
âœ… Comprensione nativa di entitÃ , servizi, automazioni HA  
âœ… Ottima comprensione del contesto  
âœ… Seguire istruzioni complesse  
âœ… Multilingua eccellente (IT, EN, ES, FR, DE, etc.)  
âœ… Dimensione ragionevole (~2 GB)  
âœ… Context window grande (4096 tokens)  

### Soluzione 2: ModalitÃ  Prompt Semplificato (Per Gemma 1B)

Se **non puoi usare un modello piÃ¹ grande**, ho aggiunto una modalitÃ  prompt ultra-semplificata:

1. **Apri Configuration dell'addon**
2. **Aggiungi questa opzione:**

```yaml
simple_prompt_mode: true
```

3. **Salva e Riavvia**

Questa modalitÃ :
- Rimuove istruzioni complesse
- Usa un prompt minimalista
- Aumenta le probabilitÃ  che il modello capisca

âš ï¸ **Nota**: Anche con prompt semplificato, Gemma 1B potrebbe non funzionare bene.

### Soluzione 3: Modelli Alternativi

Se Llama 3.2 3B non funziona sul tuo hardware:

#### Opzione A: Mistral 7B (migliore qualitÃ , piÃ¹ RAM)
```yaml
model_url: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/mistral-7b-instruct-v0.3.Q4_K_M.gguf
model_name: mistral-7b-instruct-v0.3.Q4_K_M
context_size: 8192
```
- Dimensione: ~4.4 GB
- RAM necessaria: 8 GB
- Eccellente per istruzioni complesse

#### Opzione B: Gemma 2 2B (compromesso)
```yaml
model_url: https://huggingface.co/google/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf
model_name: gemma-2-2b-it-Q4_K_M
context_size: 2048
```
- Dimensione: ~1.5 GB
- RAM necessaria: 3 GB
- Migliore di 1B ma ancora limitato

## ðŸ“Š Confronto Modelli

| Modello | Parametri | RAM | Storage | QualitÃ  HA | VelocitÃ  |
|---------|-----------|-----|---------|------------|----------|
| **Gemma 3 1B** | 1B | 2 GB | 1 GB | âŒ Scarsa | âš¡âš¡âš¡ Veloce |
| **Gemma 2 2B** | 2B | 3 GB | 1.5 GB | âš ï¸ Limitata | âš¡âš¡ Buona |
| **Llama 3.2 3B** â­ | 3B | 4 GB | 2 GB | âœ… Ottima | âš¡âš¡ Buona |
| **Mistral 7B** | 7B | 8 GB | 4.4 GB | âœ…âœ… Eccellente | âš¡ Media |
| **Phi-3 14B** | 14B | 12 GB | 7.9 GB | âœ…âœ…âœ… Superiore | ðŸŒ Lenta |

## ðŸ§ª Test

Dopo aver cambiato modello, testa con:

```bash
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Quali luci sono accese in casa?",
    "include_entities": true
  }'
```

**Risposta attesa:**
```json
{
  "response": "In base allo stato attuale della casa, le seguenti luci sono accese: [lista luci con stato 'on']",
  "usage": {...}
}
```

**Risposta ERRATA (modello troppo piccolo):**
```json
{
  "response": "Come modello linguistico, non ho accesso a informazioni personali...",
  "usage": {...}
}
```

## ðŸ“ Note Tecniche

### PerchÃ© i modelli piccoli falliscono?

1. **Context Window Limitato**: 2048 tokens potrebbero non bastare per system prompt + entitÃ  HA + conversazione
2. **CapacitÃ  di Reasoning Limitata**: <3B parametri non bastano per "ragionare" su stati delle entitÃ 
3. **Instruction-Following Debole**: Non seguono fedelmente le istruzioni nel system prompt
4. **Overflow del Context**: Se troppe entitÃ , il context "satura" e il modello perde le istruzioni iniziali

### PerchÃ© Llama 3.2 3B Funziona?

1. **3B parametri**: CapacitÃ  cognitive sufficienti per task complessi
2. **4096 tokens context**: Spazio per system prompt + molte entitÃ  + conversazione
3. **Training avanzato**: Ottimizzato per instruction-following
4. **Multilingua nativo**: Supporto eccellente per italiano
5. **Architettura moderna**: Llama 3.x Ã¨ state-of-the-art

## ðŸ” Debug

Se anche con Llama 3.2 3B non funziona:

1. **Verifica le entitÃ  HA siano accessibili:**
```bash
curl http://localhost:5000/api/ha/entities
```

2. **Verifica il prompt contenga le entitÃ :**
```bash
curl http://localhost:5000/api/debug/prompt
```

3. **Controlla i log dell'addon:**
```bash
docker logs addon_local_llamacpp_llm
```

4. **Verifica SUPERVISOR_TOKEN:**
```bash
echo $SUPERVISOR_TOKEN
```

## ðŸ“š Riferimenti

- **Lista Completa Modelli**: Vedi `RECOMMENDED_MODELS.md`
- **Guida Debug**: Vedi `DEBUG_CONTEXT.md`
- **Configurazione Avanzata**: Vedi `README.md`

## âœ¨ Raccomandazione Finale

**Passa a Llama 3.2 3B Instruct** - Ãˆ il miglior compromesso tra:
- QualitÃ  delle risposte
- Uso RAM/Storage
- VelocitÃ  di risposta
- Supporto multilingua

Con questo modello, l'integrazione Home Assistant funzionerÃ  perfettamente! ðŸŽ‰
