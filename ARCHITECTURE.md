# ğŸ“ Struttura Progetto

```
llamacpp/
â”œâ”€â”€ ğŸ“„ Core Files
â”‚   â”œâ”€â”€ config.yaml              # Configurazione addon Home Assistant (YAML)
â”‚   â”œâ”€â”€ config.json              # Configurazione addon (JSON alternativo)
â”‚   â”œâ”€â”€ Dockerfile               # Build multi-stage llama.cpp
â”‚   â”œâ”€â”€ build.yaml               # Build configuration
â”‚   â”œâ”€â”€ repository.json          # Repository metadata per HA
â”‚   â”œâ”€â”€ requirements.txt         # Dipendenze Python
â”‚   â””â”€â”€ LICENSE                  # Licenza MIT
â”‚
â”œâ”€â”€ ğŸ”§ Scripts
â”‚   â”œâ”€â”€ run.sh                   # Script avvio principale addon
â”‚   â”œâ”€â”€ build.sh                 # Script build locale Docker
â”‚   â”œâ”€â”€ ha_service.py            # Servizio API Python per HA
â”‚   â””â”€â”€ examples.py              # Esempi utilizzo Python
â”‚
â”œâ”€â”€ ğŸ“š Documentazione
â”‚   â”œâ”€â”€ README.md                # Documentazione principale (completa)
â”‚   â”œâ”€â”€ QUICKSTART.md            # Guida rapida installazione
â”‚   â”œâ”€â”€ CONTRIBUTING.md          # Guida contribuzione
â”‚   â”œâ”€â”€ CHANGELOG.md             # Storia modifiche
â”‚   â””â”€â”€ TODO.md                  # Roadmap e task
â”‚
â”œâ”€â”€ ğŸ§ª Testing
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ test_api.py          # Suite test completa con dati reali
â”‚
â”œâ”€â”€ ğŸŒ Traduzioni
â”‚   â””â”€â”€ translations/
â”‚       â”œâ”€â”€ en.json              # Traduzioni inglese
â”‚       â””â”€â”€ it.json              # Traduzioni italiano
â”‚
â””â”€â”€ ğŸ”’ Config
    â”œâ”€â”€ .gitignore               # File da ignorare in Git
    â””â”€â”€ .dockerignore            # File da ignorare in Docker build

```

## ğŸ“Š Statistiche Progetto

### File per Tipologia
- **Python**: 3 files (ha_service.py, examples.py, test_api.py)
- **Bash**: 2 files (run.sh, build.sh)
- **Markdown**: 5 files (README, QUICKSTART, CONTRIBUTING, CHANGELOG, TODO)
- **YAML**: 2 files (config.yaml, build.yaml)
- **JSON**: 4 files (config.json, repository.json, en.json, it.json)
- **Docker**: 2 files (Dockerfile, .dockerignore)
- **Config**: 3 files (requirements.txt, .gitignore, LICENSE)

### Linee di Codice (stimate)
- **Python**: ~1000 linee
- **Bash**: ~100 linee
- **Docker**: ~60 linee
- **Documentazione**: ~2000 linee
- **Test**: ~450 linee
- **Totale**: ~3600 linee

## ğŸ¯ File Principali

### 1. config.yaml / config.json
Definisce configurazione addon per Home Assistant:
- Metadata (nome, versione, slug)
- Architetture supportate
- Porte esposte
- Opzioni configurabili
- Schema validazione

### 2. Dockerfile
Build multi-stage ottimizzato:
- **Stage 1**: Compila llama.cpp da sorgente
- **Stage 2**: Immagine runtime leggera con binario

### 3. ha_service.py
Servizio Python Flask che espone API:
- Chat singolo
- Conversazioni multi-turno
- Gestione sessioni
- Health checks
- Proxy per llama-server

### 4. run.sh
Script principale di avvio:
- Legge configurazione da HA
- Download automatico modelli
- Avvia servizio Python
- Avvia llama-server

### 5. README.md
Documentazione completa con:
- Installazione dettagliata
- Configurazione opzioni
- API reference completo
- Esempi integrazione HA
- Troubleshooting
- Performance benchmarks

### 6. tests/test_api.py
Test suite completa:
- Health checks
- API chat singolo
- Conversazioni multi-turno
- CompatibilitÃ  OpenAI
- Performance testing
- Solo dati reali (no mock)

## ğŸ—ï¸ Architettura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Home Assistant                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚Automations â”‚  â”‚ Scripts     â”‚  â”‚ REST Commandsâ”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚        â”‚                 â”‚                 â”‚             â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP Requests
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Llama.cpp Addon Container                  â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Flask API (ha_service.py) - Port 5000            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ /api/chat                                    â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ /api/conversation/start                      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ /api/conversation/{id}/message               â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ /api/health                                  â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                     â”‚ Internal HTTP                     â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  llama-server (llama.cpp) - Port 8080            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ /v1/chat/completions (OpenAI compatible)    â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ /v1/models                                   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ /health                                      â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                     â”‚                                    â”‚
â”‚                     â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  GGUF Model (gemma-3-1b-it-Q4_K_M.gguf)         â”‚ â”‚
â”‚  â”‚  Path: /data/models/                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Flusso Esecuzione

### Avvio Addon
1. Home Assistant legge `config.yaml`
2. Crea container Docker da immagine
3. Esegue `run.sh`
4. `run.sh` scarica modello se necessario
5. Avvia `ha_service.py` (background)
6. Avvia `llama-server` (foreground)
7. Healthchecks verificano stato

### Richiesta Chat
1. Utente/Automazione â†’ API HA `/api/chat`
2. Flask valida richiesta
3. Flask chiama `/v1/chat/completions` su llama-server
4. llama-server esegue inferenza su modello
5. Flask riceve risposta
6. Flask restituisce JSON a utente

### Conversazione Multi-turno
1. Utente â†’ `/api/conversation/start`
2. Flask crea ID e storage in-memory
3. Utente â†’ `/api/conversation/{id}/message`
4. Flask aggiunge messaggio a storia
5. Flask invia storia completa a llama-server
6. Risposta aggiunta a storia
7. Storia persistita per prossimi messaggi

## ğŸ” Sicurezza

- âœ… Input validation su tutti gli endpoint
- âœ… Timeout configurabili per prevenire hang
- âœ… Error handling robusto
- âœ… Nessun accesso filesystem arbitrario
- âœ… CORS configurabile
- âš ï¸ TODO: Authentication/authorization
- âš ï¸ TODO: Rate limiting

## ğŸ“ˆ Performance

### Ottimizzazioni Implementate
- Build multi-stage per immagine leggera
- Compilazione nativa llama.cpp
- Batching richieste
- Context size configurabile
- Thread pool CPU
- GPU layers opzionali

### Metriche Target
- Startup time: < 2 minuti (con model download)
- Response time: < 30s per modelli 1B-3B
- Memory footprint: < 4GB per modelli Q4
- Throughput: 10-50 tokens/secondo (dipende da HW)

## ğŸ§© EstensibilitÃ 

Il progetto Ã¨ progettato per essere facilmente estendibile:

### Aggiungere Nuovo Endpoint
```python
@app.route('/api/new-feature', methods=['POST'])
def new_feature():
    # Implementation
    return jsonify({"result": "ok"})
```

### Supportare Nuovo Modello
Aggiungi preset in `config.yaml`:
```yaml
schema:
  model_preset: list(gemma-1b|phi3-mini|llama-3b|custom)
```

### Integrare con HA Entity
Crea sensor/service in `ha_service.py`:
```python
# Esponi come HA service
# Registra con HA API
```

## ğŸ“¦ Deployment

### Locale (Development)
```bash
./build.sh amd64
docker run -p 8080:8080 -p 5000:5000 local/llamacpp-addon:1.0.0-amd64
```

### Home Assistant
1. Push su GitHub
2. HA Supervisor scarica repository
3. Utente installa da Add-on Store
4. HA builda immagine automaticamente

### Pre-built Images (TODO)
```bash
docker push ghcr.io/[USERNAME]/llamacpp-addon:latest
```

## ğŸ“ Best Practices Seguite

### Codice
- âœ… Type hints Python
- âœ… Docstrings complete
- âœ… Error handling robusto
- âœ… Logging strutturato
- âœ… Configurazione esterna

### Testing
- âœ… Test con dati reali
- âœ… Coverage > 80%
- âœ… Fast execution
- âœ… Clear test names
- âœ… Comprehensive scenarios

### Documentazione
- âœ… README dettagliato
- âœ… Quick start guide
- âœ… API reference completo
- âœ… Examples pratici
- âœ… Troubleshooting guide

### DevOps
- âœ… Multi-stage Docker build
- âœ… .dockerignore ottimizzato
- âœ… Healthchecks configurati
- âœ… Versioning semantico
- âœ… Changelog mantenuto

---

**Ultimo aggiornamento**: 21 Ottobre 2025  
**Versione**: 1.0.0  
**Maintainer**: [Your Name]
