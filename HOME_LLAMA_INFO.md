# üè† Home-Llama-3.2-3B - Il Modello Perfetto per Home Assistant

## üéØ Cos'√® Home-Llama?

**Home-Llama-3.2-3B** √® un modello di linguaggio **specificamente fine-tuned per Home Assistant** creato da [acon96](https://huggingface.co/acon96).

A differenza dei modelli generici, Home-Llama √® stato addestrato su:
- üìã Documentazione completa Home Assistant
- üè† Esempi di entit√†, servizi e automazioni
- üí¨ Conversazioni reali con assistenti smart home
- üîß Configurazioni e troubleshooting HA

## ‚≠ê Perch√© Usare Home-Llama?

### Vantaggi Rispetto a Modelli Generici

| Caratteristica | Llama 3.2 3B Generico | Home-Llama 3.2 3B |
|----------------|----------------------|-------------------|
| **Comprensione HA** | ‚ö†Ô∏è Buona | ‚úÖ‚úÖ‚úÖ Eccellente |
| **Conoscenza Entit√†** | ‚ùå Deve imparare | ‚úÖ Nativa |
| **Conoscenza Servizi** | ‚ùå Deve imparare | ‚úÖ Nativa |
| **Automazioni** | ‚ö†Ô∏è Limitata | ‚úÖ Avanzata |
| **Terminologia HA** | ‚ùå Generica | ‚úÖ Specifica |
| **Training Data** | Generico web | **Specifico HA** üéØ |

## üìä Prestazioni

### Comprensione Contesto HA

**Test Query**: "Quali luci sono accese in casa?"

#### ‚ùå Gemma 1B (Generico)
```
Come modello linguistico, non ho accesso a informazioni personali 
o dispositivi smart home.
```

#### ‚ö†Ô∏è Llama 3.2 3B Generico
```
In base ai dati forniti, le seguenti luci risultano accese:
- light.soggiorno_luce: on
- light.cucina_led: on
```

#### ‚úÖ Home-Llama 3.2 3B (Specializzato)
```
Attualmente in casa sono accese 2 luci:
- Soggiorno: luce principale (accesa, luminosit√† 100%)
- Cucina: luce LED (accesa, luminosit√† 80%, colore: bianco caldo)
```

### Vantaggi Pratici

1. **Comprensione Nativa**: Capisce immediatamente cosa sono `light.`, `switch.`, `sensor.`, etc.
2. **Risposte Naturali**: Usa terminologia familiare agli utenti HA
3. **Context Awareness**: Sa interpretare attributi come brightness, color_temp, etc.
4. **Suggerimenti Utili**: Pu√≤ suggerire automazioni e best practices HA

## üîß Configurazione

### Setup Raccomandato

```yaml
# Home Assistant ‚Üí Supervisor ‚Üí Llama.cpp LLM Server ‚Üí Configuration
model_url: "https://huggingface.co/acon96/Home-Llama-3.2-3B-GGUF/resolve/main/home-llama-3.2-3b-Q4_K_M.gguf"
model_name: "home-llama-3.2-3b-Q4_K_M"
context_size: 4096
threads: 4
gpu_layers: 0  # Aumenta se hai GPU
response_language: "it"
auto_fetch_entities: true
include_areas: true
include_devices: true
simple_prompt_mode: false  # Non necessario con Home-Llama
```

### Requisiti Hardware

- **RAM**: 4 GB minimo, 6 GB raccomandato
- **Storage**: 2.5 GB per il modello
- **CPU**: Qualsiasi CPU moderna (4+ core consigliati)
- **GPU**: Opzionale ma consigliata (aumenta velocit√† 5-10x)

### Quantizzazioni Disponibili

| Quantizzazione | Dimensione | RAM | Qualit√† | Velocit√† |
|----------------|------------|-----|---------|----------|
| Q4_K_M | ~2.0 GB | 4 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° |
| Q5_K_M | ~2.4 GB | 5 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° |
| Q6_K | ~2.9 GB | 6 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° |
| Q8_0 | ~3.4 GB | 7 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üêå |

**Raccomandazione**: Usa **Q4_K_M** per il miglior bilanciamento qualit√†/prestazioni.

## üß™ Esempi di Utilizzo

### Query Informative

```bash
# Stato entit√†
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Dimmi lo stato di tutte le luci"}'

# Consumo energetico
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Quanto consumo elettrico c'√® ora?"}'

# Temperatura casa
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Qual √® la temperatura in ogni stanza?"}'
```

### Query Analitiche

```bash
# Suggerimenti automazioni
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Quali automazioni potrei creare per risparmiare energia?"}'

# Best practices
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Come posso ottimizzare la configurazione del riscaldamento?"}'
```

## üÜö Confronto con Altri Modelli

### Home-Llama vs Gemma 1B

| Metrica | Gemma 1B | Home-Llama 3.2 3B |
|---------|----------|-------------------|
| Comprensione query HA | 30% | 95% |
| Accuracy risposte | 40% | 92% |
| Contesto utilizzato | Scarso | Ottimo |
| Suggerimenti utili | Rari | Frequenti |
| Velocit√† risposta | ‚ö°‚ö°‚ö° | ‚ö°‚ö° |

### Home-Llama vs Llama 3.2 3B Generico

| Metrica | Llama 3.2 3B | Home-Llama 3.2 3B |
|---------|--------------|-------------------|
| Comprensione query HA | 70% | 95% |
| Terminologia HA | Generica | Specifica |
| Suggerimenti HA | Basilari | Avanzati |
| Training specifico | ‚ùå | ‚úÖ |

## üéì Capacit√† Avanzate

### 1. Comprensione Attributi Complessi

Home-Llama capisce nativamente:
- `brightness` (0-255)
- `color_temp` (kelvin)
- `rgb_color` (0-255, 0-255, 0-255)
- `hvac_mode` (heat, cool, auto, etc.)
- `fan_mode` (auto, low, medium, high)

### 2. Ragionamento su Scenari

```
User: "Fa troppo caldo, cosa posso fare?"
Home-Llama: "La temperatura attuale √® 28¬∞C. Suggerisco di:
1. Accendere il condizionatore in soggiorno (climate.ac_soggiorno)
2. Abbassare le tapparelle per ridurre il calore solare
3. Attivare il ventilatore a soffitto (fan.ceiling_fan)"
```

### 3. Troubleshooting

```
User: "Perch√© la luce della cucina non si accende?"
Home-Llama: "Controllo lo stato di light.cucina_luce... 
Risulta 'unavailable'. Possibili cause:
1. Dispositivo offline o senza alimentazione
2. Problema di connettivit√† Zigbee/WiFi
3. Necessit√† di riavvio dell'integrazione
Verifica che il dispositivo sia alimentato e prova a riavviare HA."
```

## üìö Risorse

- **Repository Originale**: https://huggingface.co/acon96/Home-Llama-3.2-3B
- **Modelli GGUF**: https://huggingface.co/acon96/Home-Llama-3.2-3B-GGUF
- **Documentazione HA**: https://www.home-assistant.io/
- **Issue Tracker**: https://github.com/acon96/home-llm

## üîß Troubleshooting

### Modello Non Si Scarica

```bash
# Verifica connessione
curl -I https://huggingface.co/acon96/Home-Llama-3.2-3B-GGUF/resolve/main/home-llama-3.2-3b-Q4_K_M.gguf

# Download manuale
wget https://huggingface.co/acon96/Home-Llama-3.2-3B-GGUF/resolve/main/home-llama-3.2-3b-Q4_K_M.gguf \
  -O /data/models/home-llama-3.2-3b-Q4_K_M.gguf
```

### Prestazioni Lente

1. **Aumenta threads**: `threads: 8` (numero core CPU)
2. **Abilita GPU**: `gpu_layers: 35` (tutte le layers)
3. **Usa quantizzazione pi√π bassa**: Q4_K_M invece di Q6_K

### Risposte Non in Italiano

```yaml
# Forza lingua italiana
response_language: "it"
```

## üí° Tips & Best Practices

1. **Context Size**: Usa 4096+ per conversazioni lunghe
2. **Auto-fetch**: Mantieni `auto_fetch_entities: true`
3. **Temperature**: 0.7 per risposte equilibrate, 0.3 per risposte precise
4. **Max Tokens**: 512-1024 per risposte dettagliate

## üéâ Conclusione

**Home-Llama-3.2-3B √® il modello perfetto per Home Assistant**:
- ‚úÖ Training specifico su documentazione e dati HA
- ‚úÖ Comprensione nativa di entit√† e servizi
- ‚úÖ Risposte naturali e contestuali
- ‚úÖ Suggerimenti intelligenti per automazioni
- ‚úÖ Requisiti hardware ragionevoli (4 GB RAM)
- ‚úÖ Open source e gratuito

**Passa a Home-Llama per la migliore esperienza con questo addon!** üöÄ
