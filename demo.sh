#!/bin/bash
# Quick Demo Script - Testa l'addon localmente

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘   Llama.cpp Home Assistant Addon - Demo Script           â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Check requirements
echo "â†’ Verifico requisiti..."

if ! command -v docker &> /dev/null; then
    echo "âœ— Docker non trovato. Installalo prima di continuare."
    exit 1
fi

if ! command -v curl &> /dev/null; then
    echo "âœ— curl non trovato. Installalo prima di continuare."
    exit 1
fi

echo "âœ“ Docker trovato"
echo "âœ“ curl trovato"
echo ""

# Build
echo "â†’ Build immagine Docker..."
echo "  (Questo puÃ² richiedere 10-15 minuti la prima volta)"
echo ""

./build.sh amd64

if [ $? -ne 0 ]; then
    echo "âœ— Build fallita"
    exit 1
fi

echo ""
echo "âœ“ Build completata!"
echo ""

# Run
echo "â†’ Avvio container..."
echo ""

docker run -d \
    --name llamacpp-addon-demo \
    -p 8080:8080 \
    -p 5000:5000 \
    -e MODEL_URL="https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf" \
    -e MODEL_NAME="gemma-3-1b-it-Q4_K_M" \
    -e CONTEXT_SIZE="2048" \
    -e THREADS="4" \
    local/llamacpp-addon:1.0.0-amd64

if [ $? -ne 0 ]; then
    echo "âœ— Avvio container fallito"
    exit 1
fi

echo "âœ“ Container avviato!"
echo ""

# Wait for startup
echo "â†’ Attendo avvio servizi..."
echo "  (Download modello + inizializzazione, ~2-3 minuti)"
echo ""

for i in {1..60}; do
    if curl -s http://localhost:5000/api/health > /dev/null 2>&1; then
        echo "âœ“ Servizi pronti!"
        break
    fi
    
    if [ $i -eq 60 ]; then
        echo "âœ— Timeout: servizi non pronti dopo 60 secondi"
        echo "  Controlla i log: docker logs llamacpp-addon-demo"
        exit 1
    fi
    
    echo -n "."
    sleep 2
done

echo ""
echo ""

# Test API
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                    TEST API                               â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

echo "â†’ Test 1: Health Check"
curl -s http://localhost:5000/api/health | python3 -m json.tool
echo ""
echo ""

echo "â†’ Test 2: Chat Singolo"
echo "  Domanda: 'Ciao! Rispondi con una sola parola.'"
curl -s -X POST http://localhost:5000/api/chat \
    -H "Content-Type: application/json" \
    -d '{"message": "Ciao! Rispondi con una sola parola.", "max_tokens": 20}' \
    | python3 -m json.tool
echo ""
echo ""

echo "â†’ Test 3: Avvia Conversazione"
CONV_ID=$(curl -s -X POST http://localhost:5000/api/conversation/start \
    -H "Content-Type: application/json" \
    -d '{"system_prompt": "Sei un assistente conciso."}' \
    | python3 -c "import sys, json; print(json.load(sys.stdin)['conversation_id'])")

echo "  Conversazione ID: $CONV_ID"
echo ""

echo "â†’ Test 4: Messaggio in Conversazione"
echo "  Domanda: 'Che ore sono?'"
curl -s -X POST "http://localhost:5000/api/conversation/$CONV_ID/message" \
    -H "Content-Type: application/json" \
    -d '{"message": "Che ore sono?", "max_tokens": 50}' \
    | python3 -m json.tool
echo ""
echo ""

# Cleanup
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                    CLEANUP                                â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

read -p "Vuoi fermare e rimuovere il container? (y/n) " -n 1 -r
echo ""

if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "â†’ Fermo container..."
    docker stop llamacpp-addon-demo
    
    echo "â†’ Rimuovo container..."
    docker rm llamacpp-addon-demo
    
    echo "âœ“ Cleanup completato!"
else
    echo "Container lasciato in esecuzione."
    echo ""
    echo "Per fermarlo manualmente:"
    echo "  docker stop llamacpp-addon-demo"
    echo "  docker rm llamacpp-addon-demo"
fi

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                    DEMO COMPLETATA!                       â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“š Per maggiori informazioni:"
echo "  - README.md           Documentazione completa"
echo "  - QUICKSTART.md       Guida rapida"
echo "  - examples.py         Esempi Python"
echo "  - tests/test_api.py   Test suite"
echo ""
